{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1683701622980
        }
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "## Either get environment variables, or a fallback name, which is the second parameter.\n",
        "## Currently, fill in the fallback values. Later on, we will make sure to work with Environment values. So we're already preparing for it in here!\n",
        "workspace_name = os.environ.get('WORKSPACE', 'mlops-nathan')\n",
        "subscription_id = os.environ.get('SUBSCRIPTION_ID', '7c50f9c3-289b-4ae0-a075-08784b3b9042')\n",
        "resource_group = os.environ.get('RESOURCE_GROUP', 'mlops')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1683701623388
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1683701624170
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Class FeatureStoreOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class FeatureSetOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class FeatureStoreEntityOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
          ]
        }
      ],
      "source": [
        "ml_client = MLClient(\n",
        "    credential, subscription_id, resource_group, workspace_name\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare a Virtual PC if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1683701692502
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You already have a machine named cpu-nathan, we'll reuse it as is.\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "cpu_compute_target = \"cpu-nathan\"\n",
        "\n",
        "# let's see if the compute target already exists\n",
        "cpu_machine = ml_client.compute.get(cpu_compute_target)\n",
        "print(\n",
        "    f\"You already have a machine named {cpu_compute_target}, we'll reuse it as is.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1683701627246
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment with name aml-Pillow is registered to workspace, the environment version is 0.1.0\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "import os\n",
        "\n",
        "custom_env_name = \"aml-Pillow\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Image Processing (with Pillow)\",\n",
        "    tags={\"Pillow\": \"0.0.1\"},\n",
        "    conda_file=os.path.join(\"components\", \"dataprep\", \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"0.1.0\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1683701627365
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1683701627464
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_prep_src_dir = \"./components/dataprep\"\n",
        "os.makedirs(data_prep_src_dir, exist_ok=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1683556923402
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./components/dataprep/dataprep.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {data_prep_src_dir}/dataprep.py\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import mlflow\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--output_data\", type=str, help=\"path to output data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.data)\n",
        "    print(\"output folder:\", args.output_data)\n",
        "\n",
        "    output_dir = args.output_data\n",
        "    size = (64, 64) # Later we can also pass this as a property\n",
        "\n",
        "\n",
        "    for file in glob(args.data + \"/*.jpg\"):\n",
        "        img = Image.open(file)\n",
        "        img_resized = img.resize(size)\n",
        "\n",
        "        # Save the resized image to the output directory\n",
        "        output_file = os.path.join(output_dir, os.path.basename(file))\n",
        "        img_resized.save(output_file)\n",
        "\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1683704215484
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "data_prep_component = command(\n",
        "    name=\"data_prep_image_resize\",\n",
        "    display_name=\"Data preparation, Image Resizing\",\n",
        "    description=\"Reads a data asset of images and preprocesses them by resizing them to 64 to 64.\",\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_folder\"),\n",
        "    },\n",
        "    outputs={\n",
        "        \"output_data\": Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
        "    },\n",
        "    # The source folder of the component\n",
        "    code=data_prep_src_dir,\n",
        "    command=\"\"\"python dataprep.py \\\n",
        "            --data ${{inputs.data}} \\\n",
        "            --output_data ${{outputs.output_data}} \\\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1683704292690
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Component data_prep_image_resize with Version 2023-05-15-07-10-44-9498611 is registered\n"
          ]
        }
      ],
      "source": [
        "# Now we register the component to the workspace\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1683707979201
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1683709398166
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target,\n",
        "    description=\"Custom data_prep pipeline\",\n",
        ")\n",
        "def animal_images_preprocessing_pipeline(\n",
        "    input_version: str, # Currently we don't use these version numbers, but we will use them later on.\n",
        "    output_version: str,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    # These are the animals with the version name as a second item in the tuple\n",
        "    animals = [\n",
        "        ('pandas', \"1\"),\n",
        "        ('cats', \"1\"),\n",
        "        ('dogs', \"1\")\n",
        "    ] # They are hardcoded in here, because we should give them from another component otherwise.\n",
        "    \n",
        "    jobs = {}\n",
        "    for animal in animals:\n",
        "\n",
        "        data_prep_job = data_prep_component(\n",
        "            data=Input(\n",
        "                type=\"uri_folder\",\n",
        "                path=f\"azureml:{animal[0]}:{[animal[1]]}\"\n",
        "            ),\n",
        "        )\n",
        "        \n",
        "        output_name = animal[0] + \"_resized\"\n",
        "        output_path = \"azureml://subscriptions/7c50f9c3-289b-4ae0-a075-08784b3b9042/resourcegroups/mlops/workspaces/mlops-nathan/datastores/workspaceblobstore/paths/processed_animals/\" + animal[0]\n",
        "\n",
        "        data_prep_job.outputs.output_data = Output(\n",
        "            type=\"uri_folder\",\n",
        "            path=output_path,\n",
        "            name=output_name,\n",
        "            mode=\"rw_mount\"\n",
        "        )\n",
        "\n",
        "        jobs[animal[0]] = data_prep_job\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        k: v.outputs.output_data for k,v in jobs.items()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1683709400871
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = animal_images_preprocessing_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1683709441303
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import webbrowser\n",
        "\n",
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"image_preprocessing_pipeline\",\n",
        ")\n",
        "# open the pipeline in web browser\n",
        "webbrowser.open(pipeline_job.studio_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./components/dataprep/traintestsplit.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {data_prep_src_dir}/traintestsplit.py\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import mlflow\n",
        "from glob import glob\n",
        "import math\n",
        "import random\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    SEED = 42\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--datasets\", type=str, nargs=\"+\", help=\"All the datasets to combine\")\n",
        "    parser.add_argument(\"--training_data_output\", type=str, help=\"path to training output data\")\n",
        "    parser.add_argument(\"--testing_data_output\", type=str, help=\"path to testing output data\")\n",
        "    parser.add_argument(\"--split_size\", type=int, help=\"Percentage to use as Testing data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.datasets)\n",
        "    print(\"Training folder:\", args.training_data_output)\n",
        "    print(\"Testing folder:\", args.testing_data_output)\n",
        "    print(\"Split size:\", args.split_size)\n",
        "\n",
        "    train_test_split_factor = args.split_size / 100 # Alias\n",
        "    datasets = args.datasets\n",
        "\n",
        "    training_datapaths = []\n",
        "    testing_datapaths = []\n",
        "\n",
        "\n",
        "    for dataset in datasets:\n",
        "        animal_images = glob(dataset + \"/*.jpg\")\n",
        "        print(f\"Found {len(animal_images)} images for {dataset}\")\n",
        "\n",
        "        ## Concatenate the names for the animal_name and the img_path. Don't put a / between, because the img_path already contains that\n",
        "        ## animal_images = [(default_datastore, f'processed_animals/{animal_name}{img_path}') for img_path in animal_images] # Make sure the paths are actual DataPaths\n",
        "    \n",
        "        random.seed(SEED) # Use the same random seed as I use and defined in the earlier cells\n",
        "        random.shuffle(animal_images) # Shuffle the data so it's randomized\n",
        "\n",
        "        ## Testing images\n",
        "        amount_of_test_images = math.ceil(len(animal_images) * train_test_split_factor) # Get a small percentage of testing images\n",
        "\n",
        "        animal_test_images = animal_images[:amount_of_test_images]\n",
        "        animal_training_images = animal_images[amount_of_test_images:]\n",
        "\n",
        "        # Add them all to the other ones\n",
        "        testing_datapaths.extend(animal_test_images)\n",
        "        training_datapaths.extend(animal_training_images)\n",
        "\n",
        "        print(testing_datapaths[:5])\n",
        "\n",
        "        # Write the data to the output\n",
        "        for img in animal_test_images:\n",
        "            # Open the img, which is a string filepath, then save it to the args.testing_data_output directory\n",
        "            with open(img, \"rb\") as f:\n",
        "                with open(os.path.join(args.testing_data_output, os.path.basename(img)), \"wb\") as f2:\n",
        "                    f2.write(f.read())\n",
        "\n",
        "        for img in animal_training_images:\n",
        "            # Open the img, which is a string filepath, then save it to the args.testing_data_output directory\n",
        "            with open(img, \"rb\") as f:\n",
        "                with open(os.path.join(args.training_data_output, os.path.basename(img)), \"wb\") as f2:\n",
        "                    f2.write(f.read())\n",
        "       \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "data_split_component = command(\n",
        "    name=\"data_split\",\n",
        "    display_name=\"Data Splitting to Train and Test\",\n",
        "    description=\"Reads a data asset of images and combines them into a training and testing dataset\",\n",
        "    inputs={\n",
        "        \"animal_1\": Input(type=\"uri_folder\"),\n",
        "        \"animal_2\": Input(type=\"uri_folder\"),\n",
        "        \"animal_3\": Input(type=\"uri_folder\"),\n",
        "        \"train_test_split_factor\": Input(type=\"number\") # The percentage of the data to use as testing data, always a positive value\n",
        "    },\n",
        "    outputs={\n",
        "        \"training_data\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        \"testing_data\": Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
        "    },\n",
        "    # The source folder of the component\n",
        "    code=data_prep_src_dir,\n",
        "    command=\"\"\"python traintestsplit.py \\\n",
        "            --datasets ${{inputs.animal_1}} ${{inputs.animal_2}} ${{inputs.animal_3}} \\\n",
        "            --training_data ${{outputs.training_data}} \\\n",
        "            --testing_data ${{outputs.testing_data}} \\\n",
        "            --split_size ${{inputs.train_test_split_factor}}\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading dataprep (0.0 MBs): 100%|██████████| 4374/4374 [00:00<00:00, 14333.70it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Component data_split with Version 2023-05-15-07-33-41-2937143 is registered\n"
          ]
        }
      ],
      "source": [
        "# Now we register the component to the workspace\n",
        "data_split_component = ml_client.create_or_update(data_split_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_split_component.name} with Version {data_split_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target,\n",
        "    description=\"Custom data_prep pipeline\",\n",
        ")\n",
        "def animal_images_traintest_split_pipeline(\n",
        "    train_test_split: int, # Currently we don't use these version numbers, but we will use them later on.\n",
        "    animal_1: Input,\n",
        "    animal_2: Input,\n",
        "    animal_3: Input,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    # These are the animals with the version name as a second item in the tuple\n",
        "\n",
        "    # This doesnt work yet, I tried to get it to work, but I haven't found out how it works.\n",
        "    # Combining arguments starting with \"animals_\" into a dictionary\n",
        "    animals_args = {k: v for k, v in locals().items() if k.startswith(\"animals_\")}\n",
        "\n",
        "\n",
        "    data_split_job = data_split_component(\n",
        "            animal_1=animal_1,\n",
        "            animal_2=animal_2,\n",
        "            animal_3=animal_3,\n",
        "            train_test_split_factor=train_test_split\n",
        "        )\n",
        "    \n",
        "    # Let Azure decide a unique place everytime\n",
        "    data_split_job.outputs.training_data = Output(\n",
        "        type=\"uri_folder\",\n",
        "        name=\"training_data\",\n",
        "        mode=\"rw_mount\"\n",
        "    )\n",
        "    data_split_job.outputs.testing_data = Output(\n",
        "        type=\"uri_folder\",\n",
        "        name=\"testing_data\",\n",
        "        mode=\"rw_mount\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"training_data\": data_split_job.outputs.training_data,\n",
        "        \"testing_data\": data_split_job.outputs.testing_data\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'animal_1': {'type': 'uri_folder', 'path': 'azureml:pandas_resized:2'}, 'animal_2': {'type': 'uri_folder', 'path': 'azureml:cats_resized:2'}, 'animal_3': {'type': 'uri_folder', 'path': 'azureml:dogs_resized:2'}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "version = \"2\"\n",
        "animals = [\"pandas\", \"cats\", \"dogs\"]\n",
        "\n",
        "animals_datasets = {\n",
        "    f\"animal_{i+1}\": Input(type=\"uri_folder\", path=f\"azureml:{animal}_resized:{version}\")\n",
        "    for i, animal in enumerate(animals)\n",
        "}\n",
        "\n",
        "print(animals_datasets)\n",
        "\n",
        "train_test_pipeline = animal_images_traintest_split_pipeline(\n",
        "    **animals_datasets,\n",
        "    train_test_split=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "import webbrowser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# submit the pipeline job\n",
        "train_test_pipeline_job = ml_client.jobs.create_or_update(\n",
        "    train_test_pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"image_preprocessing_pipeline\",\n",
        ")\n",
        "# open the pipeline in web browser\n",
        "webbrowser.open(train_test_pipeline_job.studio_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "training_src_dir = \"./components/training\"\n",
        "os.makedirs(training_src_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment with name aml-Tensorflow-Pillow is registered to workspace, the environment version is 0.1.1\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "import os\n",
        "\n",
        "custom_env_name = \"aml-Tensorflow-Pillow\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for AI Training (with Pillow)\",\n",
        "    tags={\"Pillow\": \"0.0.1\", \"Tensorflow\": \"2.4.1\"},\n",
        "    conda_file=os.path.join(\"components\", \"training\", \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"0.1.1\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./components/training/train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {training_src_dir}/train.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from glob import glob\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import mlflow\n",
        "\n",
        "# This time we will need our Tensorflow Keras libraries, as we will be working with the AI training now\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# This AzureML package will allow to log our metrics etc.\n",
        "from azureml.core import Run\n",
        "\n",
        "# Important to load in the utils as well!\n",
        "from utils import *\n",
        "\n",
        "\n",
        "### HARDCODED VARIABLES FOR NOW\n",
        "### TODO for the students:\n",
        "### Make sure to adapt the ArgumentParser on line 31 to include these parameters\n",
        "### You can base your answer on the lines that are already there\n",
        "\n",
        "SEED = 42\n",
        "INITIAL_LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 11\n",
        "model_name = 'animal-cnn-test'\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--training_folder', type=str, dest='training_folder', help='training folder mounting point')\n",
        "    parser.add_argument('--testing_folder', type=str, dest='testing_folder', help='testing folder mounting point')\n",
        "    parser.add_argument('--output_folder', type=str, dest='output_folder', help='Output folder')\n",
        "    parser.add_argument('--epochs', type=int, dest='epochs', help='The amount of Epochs to train')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    training_folder = args.training_folder\n",
        "    print('Training folder:', training_folder)\n",
        "\n",
        "    testing_folder = args.testing_folder\n",
        "    print('Testing folder:', testing_folder)\n",
        "\n",
        "    output_folder = args.output_folder\n",
        "    print('Testing folder:', output_folder)\n",
        "\n",
        "    MAX_EPOCHS = args.epochs\n",
        "\n",
        "    # As we're mounting the training_folder and testing_folder onto the `/mnt/data` directories, we can load in the images by using glob.\n",
        "    training_paths = glob(training_folder + \"/*.jpg\", recursive=True)\n",
        "    testing_paths = glob(testing_folder + \"/*.jpg\", recursive=True)\n",
        "\n",
        "    print(\"Training samples:\", len(training_paths))\n",
        "    print(\"Testing samples:\", len(testing_paths))\n",
        "\n",
        "    # Make sure to shuffle in the same way as I'm doing everything\n",
        "    random.seed(SEED)\n",
        "    random.shuffle(training_paths)\n",
        "    random.seed(SEED)\n",
        "    random.shuffle(testing_paths)\n",
        "\n",
        "    print(training_paths[:3]) # Examples\n",
        "    print(testing_paths[:3]) # Examples\n",
        "\n",
        "    # Parse to Features and Targets for both Training and Testing. Refer to the Utils package for more information\n",
        "    X_train = getFeatures(training_paths)\n",
        "    y_train = getTargets(training_paths)\n",
        "\n",
        "    X_test = getFeatures(testing_paths)\n",
        "    y_test = getTargets(testing_paths)\n",
        "\n",
        "    print('Shapes:')\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "    print(len(y_train))\n",
        "    print(len(y_test))\n",
        "\n",
        "    # Make sure the data is one-hot-encoded\n",
        "    LABELS, y_train, y_test = encodeLabels(y_train, y_test)\n",
        "    print('One Hot Shapes:')\n",
        "\n",
        "    print(y_train.shape)\n",
        "    print(y_test.shape)\n",
        "\n",
        "    # Create an output directory where our AI model will be saved to.\n",
        "    # Everything inside the `outputs` directory will be logged and kept aside for later usage.\n",
        "    model_path = os.path.join(output_folder, model_name)\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "\n",
        "    # Save the best model, not the last\n",
        "    cb_save_best_model = keras.callbacks.ModelCheckpoint(filepath=model_path,\n",
        "                                                            monitor='val_loss', \n",
        "                                                            save_best_only=True, \n",
        "                                                            verbose=1)\n",
        "\n",
        "    # Early stop when the val_los isn't improving for PATIENCE epochs\n",
        "    cb_early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience= PATIENCE,\n",
        "                                                verbose=1,\n",
        "                                                restore_best_weights=True)\n",
        "\n",
        "    # Reduce the Learning Rate when not learning more for 4 epochs.\n",
        "    cb_reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=4, verbose=1)\n",
        "\n",
        "    opt = tf.keras.optimizers.legacy.SGD(lr=INITIAL_LEARNING_RATE, decay=INITIAL_LEARNING_RATE / MAX_EPOCHS) # Define the Optimizer\n",
        "\n",
        "    model = buildModel((64, 64, 3), 3) # Create the AI model as defined in the utils script.\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "    # Construct & initialize the image data generator for data augmentation\n",
        "    # Image augmentation allows us to construct “additional” training data from our existing training data \n",
        "    # by randomly rotating, shifting, shearing, zooming, and flipping. This is to avoid overfitting.\n",
        "    # It also allows us to fit AI models using a Generator, so we don't need to capture the whole dataset in memory at once.\n",
        "    aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
        "                            height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "                            horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "\n",
        "    # train the network\n",
        "    history = model.fit( aug.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "                            validation_data=(X_test, y_test),\n",
        "                            steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "                            epochs=MAX_EPOCHS,\n",
        "                            callbacks=[cb_save_best_model, cb_early_stop, cb_reduce_lr_on_plateau] )\n",
        "\n",
        "    print(\"[INFO] evaluating network...\")\n",
        "    predictions = model.predict(X_test, batch_size=32)\n",
        "    print(classification_report(y_test.argmax(axis=1), predictions.argmax(axis=1), target_names=['cats', 'dogs', 'panda'])) # Give the target names to easier refer to them.\n",
        "    # If you want, you can enter the target names as a parameter as well, in case you ever adapt your AI model to more animals.\n",
        "\n",
        "    cf_matrix = confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
        "    print(cf_matrix)\n",
        "\n",
        "    ### TODO for students\n",
        "    ### Find a way to log more information to the Run context.\n",
        "\n",
        "    # Save the confusion matrix to the outputs.\n",
        "    np.save(os.path.join(output_folder, '/confusion_matrix.npy'), cf_matrix)\n",
        "\n",
        "    print(\"DONE TRAINING\")\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "training_component = command(\n",
        "    name=\"training\",\n",
        "    display_name=\"Training an AI model\",\n",
        "    description=\"Trains an AI model by inputting a lot of training and testing data.\",\n",
        "    inputs={\n",
        "        \"training_folder\": Input(type=\"uri_folder\"),\n",
        "        \"testing_folder\": Input(type=\"uri_folder\"),\n",
        "        \"epochs\": Input(type=\"number\") # The percentage of the data to use as testing data, always a positive value\n",
        "    },\n",
        "    outputs={\n",
        "        \"output_folder\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    },\n",
        "    # The source folder of the component\n",
        "    code=training_src_dir,\n",
        "    command=\"\"\"python train.py \\\n",
        "            --training_folder ${{inputs.training_folder}} \\\n",
        "            --testing_folder ${{inputs.testing_folder}} \\\n",
        "            --output_folder ${{outputs.output_folder}} \\\n",
        "            --epochs ${{inputs.epochs}} \\\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading training (0.01 MBs): 100%|██████████| 10244/10244 [00:00<00:00, 158264.55it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Component training with Version 2023-05-16-08-58-57-7385595 is registered\n"
          ]
        }
      ],
      "source": [
        "# Now we register the component to the workspace\n",
        "training_component = ml_client.create_or_update(training_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {training_component.name} with Version {training_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target,\n",
        "    description=\"Custom Animals Training pipeline\",\n",
        ")\n",
        "def animals_training_pipeline(\n",
        "    training_folder: Input, # Currently we don't use these version numbers, but we will use them later on.\n",
        "    testing_folder: Input,\n",
        "    epochs: int,\n",
        "):\n",
        "\n",
        "    training_job = training_component(\n",
        "        training_folder=training_folder,\n",
        "        testing_folder=testing_folder,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    \n",
        "    # Let Azure decide a unique place everytime\n",
        "    training_job.outputs.output_folder = Output(\n",
        "        type=\"uri_folder\",\n",
        "        name=\"output_data\",\n",
        "        mode=\"rw_mount\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"output_data\": training_job.outputs.output_folder,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "\n",
        "training_pipeline = animals_training_pipeline(\n",
        "    training_folder=Input(type=\"uri_folder\", path=f\"azureml:training_data:2\"),\n",
        "    testing_folder=Input(type=\"uri_folder\", path=f\"azureml:testing_data:12\"),\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import webbrowser\n",
        "# submit the pipeline job\n",
        "training_pipeline_job = ml_client.jobs.create_or_update(\n",
        "    training_pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"training_pipeline\",\n",
        ")\n",
        "# open the pipeline in web browser\n",
        "webbrowser.open(training_pipeline_job.studio_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "mlops",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "1489b69aef9188e253991a72f2c1dcab719183ba23071189a69ddf1bcdf6e734"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
